\subsection{Word Embedding}

Image and audio data are rich and high-dimensional, which gives their learning systems lots of things to work with. However, one of the difficulties in Natural Language Processing (NLP) is due to the sparsity of data, because normal word encodings are arbitrary to learning systems and no useful information exists between encodings of any two words. For example, in the certain encoding scheme, the word ``good'' may be encoded as {\tt Id123} while the word ``fantastic'' is encoded as {\tt Id456}. These two IDs don't mean anything special to any system even though the two words share similar meanings in lots of contexts. Word embeddings can resolve this issue to certain degree.

A word embedding is a parameterized function mapping words in certain language to high-dimensional vectors. It is grounded on the assumption that words that appear in the same contexts share semantic meaning. In the high dimensional embedding space, semantically similar words are mapped to points that are nearby each other. For example, the embeddings for ``dog'' and ``cat'' are close. In fact, words for animals are in general close to each other. In addition, words' semantic differences are captured by their distances in the embedding space. For instance, the distance vector of ``Beijing'' to ``China'' is similar to that of ``Paris'' to ``France'', and the distance vector of ``woman'' to ``man'' is similar to that of ``queen'' to ``king''. Because word embeddings are able to capture semantic relationships between words, it can provide NLP systems richer data. 

We use the {\tt word2vec} model in \cite{mikolov2013} in this term project. It comes in two flavors, the Continuous Bag-of-Words model (CBOW) and the Skip-Gram model. These two models are algorithmically similar. The difference is that, CBOW model is trained to predict a target word from its context while Skip-Gram model is trained to predict context words from target word. As an example, consider the phrase ``the cat sits on the mat''. We parse the data one word at a time, so each word gets to be the target word once. If ``mat'' is the target word, CBOW predicts ``mat'' from ``the cat sits on the'', while Skip-Gram predicts ``the'', ``cat'', ``sits'', ``on'' or ``the'' from ``mat''. CBOW model is better for smaller datasets while Skip-Gram model is better for larger ones.
