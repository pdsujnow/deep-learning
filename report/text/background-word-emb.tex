\subsection{Word Embedding}

Image and audio data are rich and high-dimensional, which gives their learning systems lots of things to work with. However, one of the difficulties in Natural Language Processing (NLP) is due to the sparsity of data, because normal word encodings are arbitrary to learning systems and no useful information exists between encodings of any two words. For example, in the certain encoding scheme, the word ``good'' may be encoded as {\tt Id123} while the word ``fantastic'' is encoded as {\tt Id456}. These two IDs don't mean anything special to any system even though the two words share similar meanings in lots of contexts. Word embeddings can resolve this issue to certain degree.

A word embedding is a parameterized function mapping words in certain language to high-dimensional vectors. It is grounded on the assumption that words that appear in the same contexts share semantic meanings. The function, which is essentially a lookup table of words to embeddings, is obtained by training a neural network with word-context data. In the high dimensional embedding space, semantically similar words are mapped to points that are nearby each other. For example, the embeddings for ``dog'' and ``cat'' are close. In fact, words for animals are in general close to each other. In addition, words' semantic differences are captured by their distances in the embedding space. For instance, the distance vector of ``Beijing'' to ``China'' is similar to that of ``Paris'' to ``France'', and the distance vector of ``woman'' to ``man'' is similar to that of ``queen'' to ``king''. Because word embeddings are able to capture semantic relationships between words, it can provide NLP systems richer data. 
