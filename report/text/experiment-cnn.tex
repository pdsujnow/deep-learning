\subsection{CNN}

\begin{table}[htb]
\centering
\begin{tabular}{| l | r |}
\hline
{\bf Hyperparameter} & {\bf Value}  \\ \hline
sentence length & 200 \\ \hline
word embedding size & 200 \\ \hline
filter window size & 1, 2 \\ \hline
number of filters per window size & 128 \\ \hline
dropout probability & 0.5 \\
\hline
\end{tabular}
\caption{Hyperparameters of CNN model}\label{tbl.cnn.param}
\end{table}

The dataset used in the experiments is from the Stanford Twitter Sentiment corpus \footnote{\tt http://help.sentiment140.com/for-students/}, which consists of 1.6 million two-class machine-labeled tweets for training, and 498 three-class hand-labeled tweets for test. We composed a smaller training set of 25,000 positive and 25,000 negative examples from the original training set, and a smaller test set consists of all the 359 positive and negative examples from the original test set. The data is preprocessed to separate word contractions, e.g. ``I've'' to ``I 've'' with extra space, and to remove punctuations and other symbols like ``\#'' and ``@''. Each sentence is then tokenized by the {\tt TweetTokenizer} provided in Python NLTK \footnote{\tt http://www.nltk.org} library and padded to 200 tokens as necessary. 

The CNN is implemented in TensorFlow, Google's deep learning library \footnote{\tt https://www.tensorflow.org}. The network structure is defined in Python, but the backend is implemented in C++, so the training and testing procedures run as C++ programs. The hyperparameters of the model are listed in Table \ref{tbl.cnn.param}. 

The skip-gram {\tt word2vec} model proposed in \cite{mikolov2013} is used for the word embedding layer. The size of the embeddings is 200. The embedding model is pre-trained using a subset of the Google News data used in \cite{mikolov2013} that consists of 17 million words, with a vocabulary of 71,291 words. After plugged into the CNN, the parameters of the {\tt word2vec} model are set untrainable so it becomes a static lookup table. There're two reasons for fixing the parameters:
\begin{enumerate}
\item To reduce the number of parameters need to be learnt.
\item Tweets contain lots of noise, making the layer trainable exposes it to the noises, which may be counterproductive. \footnote{We did test the model with the embedding layer set trainable. The performance is slightly worse.}
\end{enumerate}
Because of this layer, the vocabulary of the CNN is determined by the {\tt word2vec} model, saved as a word-to-index dictionary. During data preprocessing, each word is converted to an index according to the dictionary. 

For the convolutional layer, there can be a number of a filters with different window sizes. In order to keep our model simple and small, only two windows sizes, 1 and 2, are used, and each window size has 128 filters. 

The model is trained with data batches of 128 tweets for 10 epochs. On a computer with 2.8GHz quad-core CPU and 16GB of memory, the model can be trained and tested well under half an hour. We run the training and testing 20 times, and obtained an average accuracy of 77.72\%. Although this is not a very impressive performance, since our model has a very simple design and it is not optimized in any way, it still shows that there're lots of potentials in CNN.